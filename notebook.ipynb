{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing Hospital Efficiency with ML: Data Cleaning, XGBoost, and Predictive Modeling.\n",
    "\n",
    "In the ever-evolving landscape of healthcare, the quest to enhance patient care outcomes and elevate the quality of healthcare services is an ongoing mission. Healthcare organizations are navigating a complex web of challenges, but within these challenges, lies a remarkable opportunity – the power of data.\n",
    "\n",
    "Healthcare analytics is the compass that guides organizations towards this opportunity. It's the art of dissecting and interpreting data, employing both quantitative and qualitative techniques to unveil the hidden gems of insights and patterns within the wealth of healthcare information. Among the multitude of metrics used for performance evaluation, one vital indicator stands out - the Length of Stay (LOS) for patients.\n",
    "\n",
    "Predicting a patient's Length of Stay is akin to holding a key that unlocks a world of possibilities. It empowers hospitals to optimize their treatment plans with precision, a measure that not only reduces LOS but also minimizes infection rates among patients, staff, and visitors. In essence, it's a pathway to not just improving patient care but revolutionizing healthcare management as a whole.\n",
    "\n",
    "In this journey towards better healthcare, you play a pivotal role. You are the data virtuoso, armed with the latest tools and techniques in healthcare analytics. Your mission is to transform raw data into meaningful insights that illuminate the intricate web of patient care. Through the lens of data analysis, you decode the mysteries of patient LOS, revealing trends and patterns that hold the key to more efficient and effective healthcare delivery.\n",
    "\n",
    "Collaborating closely with the healthcare team, you craft compelling data visualizations that bring these insights to life. Your data-driven creations become the guiding stars, steering healthcare professionals towards better decision-making, enhanced care, and safer environments. While the intricacies of your work may often go unnoticed, its impact reverberates throughout the healthcare organization.\n",
    "\n",
    "In the world of healthcare analytics, you are the unsung hero, the one who helps unveil the extraordinary stories of improved patient care and streamlined healthcare management. Your dedication to data and your ability to transform it into illuminating insights contribute to the ongoing saga of healthcare excellence, making every patient's journey towards better health that much more extraordinary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Analyzing the train data\n",
    "\n",
    "Task is to take the data from the file in the root directory named train.csv and assign it to the variable 'train'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Decoding the test data\n",
    "\n",
    "Task is to take the data from the file in the root directory named test.csv and assign it to the variable 'test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Navigating the Missing Values.\n",
    "\n",
    "Calculate the count of null values in each column of the 'train' DataFrame and store the results in a Pandas Series assigned to the variable 'null_values_train'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_train = train.isnull().sum()\n",
    "\n",
    "null_values_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Conquering the Enigma of Missing Values\n",
    "\n",
    "Calculate the count of null values in each column of the 'test' DataFrame and store the results in a Pandas Series assigned to the variable 'null_values_test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_test = test.isnull().sum()\n",
    "\n",
    "null_values_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Data Healing\n",
    "\n",
    "Replace the missing values in the 'Bed Grade' and 'City_Code_Patient' columns of the 'train' DataFrame with their respective modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Bed Grade'].fillna(train['Bed Grade'].mode()[0], inplace = True)\n",
    "train['City_Code_Patient'].fillna(train['City_Code_Patient'].mode()[0], inplace = True)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Data Completeness\n",
    "\n",
    "Replace the missing values in the 'Bed Grade' and 'City_Code_Patient' columns of the 'test' DataFrame with their respective modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Bed Grade'].fillna(test['Bed Grade'].mode()[0], inplace = True)\n",
    "test['City_Code_Patient'].fillna(test['City_Code_Patient'].mode()[0], inplace = True)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Transforming 'Stay' with LabelEncoder\n",
    "\n",
    "We should encode the 'Stay' column in the 'train' DataFrame using LabelEncoder, replacing categorical values with numerical labels.\n",
    "\n",
    "This transformation is essential to enable predictive modeling and machine learning algorithms to make sense of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "train['Stay'] = le.fit_transform(train['Stay'].astype('str'))\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Charting the Unknown\n",
    "\n",
    "Task is to assign the value -1 to the 'Stay' column for all rows in the 'test' DataFrame.\n",
    "\n",
    "The reasons for this move lay hidden within the algorithmic realm, a placeholder to be eventually replaced by machine-generated predictions. As we execute this change, it symbolize the start of a new phase in their mission – the one where the algorithm would craft predictions and, in turn, determine the lengths of patients' stays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Stay'] = -1\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Data Convergence\n",
    "\n",
    "Task is to create a new DataFrame named 'df' by concatenating the 'train' and 'test' DataFrames along their rows and resetting the index with continuous numbering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], ignore_index = True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: Transforming Categories into Numbers\n",
    "\n",
    "Task is to encode the 'Hospital_type_code', 'Hospital_region_code', 'Department', 'Ward_Type', 'Ward_Facility_Code', 'Type of Admission', 'Severity of Illness', and 'Age' columns in the 'df' DataFrame using LabelEncoder, replacing categorical values with numerical labels.\n",
    "\n",
    "This transformation turns the categorical variables into numerical representations, providing a common language for the algorithms to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Hospital_type_code', 'Hospital_region_code', 'Department', 'Ward_Type', 'Ward_Facility_Code', 'Type of Admission', 'Severity of Illness', 'Age']:\n",
    "          le = LabelEncoder()\n",
    "          df[i] = le.fit_transform(df[i].astype('str'))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 11: Data Segmentation\n",
    "\n",
    "Task is to create a new DataFrame named 'train' by filtering rows in the DataFrame 'df' where the value in the 'Stay' column is not equal to -1.\n",
    "\n",
    "The \"train\" dataset now hold only the rows where the outcomes are known, and it is this dataset that would be the cornerstone for developing and validating predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['Stay']!=-1]\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 12: Preparation for Prediction\n",
    "\n",
    "The task is to create a new DataFrame named 'test' by filtering rows in the DataFrame 'df' where the value in the 'Stay' column is equal to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['Stay']==-1]\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 13: Feature Engineering for Enhanced Predictive Analysis\n",
    "\n",
    "Task is to create a new DataFrame named 'test1' by dropping the columns 'Stay', 'patientid', 'Hospital_region_code', and 'Ward_Facility_Code' from the 'test' DataFrame.\n",
    "\n",
    "With each transformation, we are enhancing the dataset, preparing it for the predictive modeling phase. The result is a dataset, \"test1,\" meticulously refined, and now poised for predictive analysis, with features engineered to uncover hidden patterns in patient stays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_countid_enocde(train, test, cols, name):\n",
    "  temp = train.groupby(cols)['case_id'].count().reset_index().rename(columns = {'case_id': name})\n",
    "  temp2 = test.groupby(cols)['case_id'].count().reset_index().rename(columns = {'case_id': name})\n",
    "  train = pd.merge(train, temp, how='left', on= cols)\n",
    "  test = pd.merge(test,temp2, how='left', on= cols)\n",
    "  train[name] = train[name].astype('float')\n",
    "  test[name] = test[name].astype('float')\n",
    "  train[name].fillna(np.median(temp[name]), inplace = True)\n",
    "  test[name].fillna(np.median(temp2[name]), inplace = True)\n",
    "  return train, test\n",
    "\n",
    "\n",
    "train, test = get_countid_enocde(train, test, ['patientid'], name = 'count_id_patient')\n",
    "train, test = get_countid_enocde(train, test,\n",
    "                                 ['patientid', 'Hospital_region_code'], name = 'count_id_patient_hospitalCode')\n",
    "train, test = get_countid_enocde(train, test,\n",
    "                                 ['patientid', 'Ward_Facility_Code'], name = 'count_id_patient_wardfacilityCode')\n",
    "\n",
    "test1 = test.drop(['Stay', 'patientid', 'Hospital_region_code', 'Ward_Facility_Code'], axis =1)\n",
    "test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 14: Sculpting the Data\n",
    "\n",
    "Task is to create a new DataFrame named 'train1' by dropping the columns 'case_id', 'patientid', 'Hospital_region_code', and 'Ward_Facility_Code' from the 'train' DataFrame.\n",
    "\n",
    "By removing these variables, we aim to reduce noise and focus on the most relevant features for predicting patient stays. The \"train1\" dataset now stood as a lean, purpose-built platform for the upcoming modeling stage, where machine learning algorithms would uncover the intricate patterns within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.drop(['case_id', 'patientid', 'Hospital_region_code', 'Ward_Facility_Code'], axis =1)\n",
    "\n",
    "train1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 15: Data Splitting for Model Mastery\n",
    "\n",
    "Task is to split the 'train1' data into training and testing sets. Create feature variables X1 by excluding the 'Stay' column and set y1 as the target variable. Split the data into training and testing sets using a 80-20 split ratio, with a random seed of 100 for reproducibility.\n",
    "\n",
    "This division of data is a fundamental step in the process, essential for training and assessing the performance of machine learning models. With the training and testing datasets now established, we are poised to embark on the final phases of their data-driven journey, where predictive modeling would unveil insights, guide decision-making, and bring the project to its ultimate conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset\n",
    "X1 = train1.drop('Stay', axis=1)\n",
    "y1 = train1['Stay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.20, random_state=100)\n",
    "\n",
    "# Displaying a preview of the train and test datasets\n",
    "print(\"Preview of X_train:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nPreview of X_test:\")\n",
    "display(X_test.head())\n",
    "\n",
    "print(\"\\nPreview of y_train:\")\n",
    "display(y_train.head().to_frame())\n",
    "\n",
    "print(\"\\nPreview of y_test:\")\n",
    "display(y_test.head().to_frame())\n",
    "\n",
    "# Summary of dataset splits\n",
    "print(\"\\nDataset Split Summary:\")\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 16: The XGBoost Model Story\n",
    "\n",
    "Task is to create an XGBoost classifier instance named 'classifier_xgb' with specified hyperparameters. Fit the classifier to the training data ('X_train' and 'y_train') and assign it to 'model_xgb'. Use the trained model to make predictions on the test data ('X_test') and store the predictions. Calculate the accuracy of the XGBoost model's predictions and store it in 'acc_score_xgb'. Round the accuracy score to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "classifier_xgb = xgboost.XGBClassifier(max_depth=4, learning_rate=0.1, n_estimators=800,\n",
    "                                  objective='multi:softmax', reg_alpha=0.5, reg_lambda=1.5,\n",
    "                                  booster='gbtree', n_jobs=4, min_child_weight=2, base_score= 0.75)\n",
    "\n",
    "\n",
    "model_xgb = classifier_xgb.fit(X_train, y_train)\n",
    "\n",
    "prediction_xgb = model_xgb.predict(X_test)\n",
    "acc_score_xgb = accuracy_score(prediction_xgb,y_test)\n",
    "\n",
    "acc_score_xgb = round(acc_score_xgb, 2)\n",
    "\n",
    "\n",
    "print(f\"\\nXGBoost Model Accuracy: {acc_score_xgb * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 17: The Final Predictive Model Transformation\n",
    "\n",
    "task is to use the trained 'classifier_xgb' model to make predictions on the 'test1' DataFrame excluding the 'case_id' column. Store the predictions in 'pred_xgb'. Create a new DataFrame named 'result_xgb' to organize the prediction results with 'pred_xgb' values added to the 'Stay' column. Assign the 'case_id' column from 'test1' to the 'case_id' column in 'result_xgb'. Reorder the columns in 'result_xgb' to have 'case_id' as the first column and 'Stay' as the second column. Replace the numeric labels in the 'Stay' column of 'result_xgb' with provided label_mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: '0-10', 1: '11-20', 2: '21-30', 3: '31-40', 4: '41-50',\n",
    "    5: '51-60', 6: '61-70', 7: '71-80', 8: '81-90', 9: '91-100',\n",
    "    10: 'More than 100 Days'\n",
    "}\n",
    "\n",
    "# Predict on test data\n",
    "pred_xgb = classifier_xgb.predict(test1.iloc[:, 1:])\n",
    "result_xgb = pd.DataFrame({'case_id': test1['case_id'], 'Stay': pred_xgb})\n",
    "result_xgb['Stay'] = result_xgb['Stay'].replace(label_mapping)\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "display(result_xgb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 18: Decoding Patient Stays\n",
    "\n",
    "Task is to group the 'result_xgb' DataFrame by unique 'Stay' values and calculate the count of unique 'case_id' values in each group. Store the result in the variable 'result'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result_xgb.groupby('Stay')['case_id'].nunique().reset_index()\n",
    "\n",
    "# Rename columns for better readability\n",
    "result.columns = ['Stay Duration', 'Number of Cases']\n",
    "\n",
    "# Display the summarized result\n",
    "print(\"\\nSummary of Stay Duration Counts:\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
